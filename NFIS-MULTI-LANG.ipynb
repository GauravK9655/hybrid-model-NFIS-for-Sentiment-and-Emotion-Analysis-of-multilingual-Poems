{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import pandas as pd\n","\n","# Define the mapping of emotions to each sentiment with intensity values\n","emotion_intensity_map = {\n","    \"Positive\": [(\"Hope\", 0), (\"Joy\", 1), (\"Love\", 2)],\n","    \"Neutral\": [(\"Calm\", 0), (\"Acceptance\", 1), (\"Peace\", 2)],\n","    \"Negative\": [(\"Sadness\", 0), (\"Grief\", 1), (\"Anger\", 2)]\n","}\n","\n","# Function to assign intensity based on sentiment and emotion\n","def assign_intensity(row):\n","    sentiment = row['Sentiment']\n","    emotion_list = emotion_intensity_map.get(sentiment, [])\n","    for emotion, intensity in emotion_list:\n","        if row['Emotion'] == emotion:\n","            return intensity\n","    return None\n","\n","# Load datasets\n","english_df = pd.read_csv('english.csv')\n","gujrati_df = pd.read_csv('gujrati.csv')\n","hindi_df = pd.read_csv('hindi.csv')\n","marathi_df = pd.read_csv('marathi.csv')\n","\n","# Apply the function to add an Intensity column to each dataset\n","english_df['Intensity'] = english_df.apply(assign_intensity, axis=1)\n","gujrati_df['Intensity'] = gujrati_df.apply(assign_intensity, axis=1)\n","hindi_df['Intensity'] = hindi_df.apply(assign_intensity, axis=1)\n","marathi_df['Intensity'] = marathi_df.apply(assign_intensity, axis=1)\n","\n","# Save the datasets with intensity\n","english_df.to_csv('english_with_intensity.csv', index=False)\n","gujrati_df.to_csv('gujrati_with_intensity.csv', index=False)\n","hindi_df.to_csv('hindi_with_intensity.csv', index=False)\n","marathi_df.to_csv('marathi_with_intensity.csv', index=False)\n"],"metadata":{"id":"DSdZ-lJADLfh","executionInfo":{"status":"ok","timestamp":1727412373215,"user_tz":-330,"elapsed":36828,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import string\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","\n","# Custom stopwords for Hindi\n","hindi_stopwords = set(['है', 'में', 'की', 'से', 'यह', 'था', 'और'])  # Add more as needed\n","\n","# Load the Hindi dataset\n","hindi_df = pd.read_csv('hindi_with_intensity.csv')\n","\n","# Function to normalize text\n","def normalize_text(text):\n","    text = text.lower()  # Lowercase the text\n","    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)  # Remove punctuation and special characters\n","    text = re.sub(r'\\d+', '', text)  # Remove numbers\n","    text = text.strip()  # Remove extra whitespace\n","    return text\n","\n","# Function to remove stopwords for Hindi\n","def remove_stopwords(text):\n","    return ' '.join([word for word in text.split() if word not in hindi_stopwords])\n","\n","# Apply text normalization and stopword removal for Hindi dataset\n","def preprocess_hindi(df):\n","    df['Sentence'] = df['Sentence'].apply(normalize_text)\n","    df['Sentence'] = df['Sentence'].apply(remove_stopwords)\n","    return df\n","\n","# Preprocess Hindi dataset\n","hindi_df = preprocess_hindi(hindi_df)\n","\n","# Function to filter sentences based on length (between 3 and 50 words)\n","def filter_sentence_length(df, column_name, min_len=3, max_len=50):\n","    df = df[df[column_name].apply(lambda x: min_len <= len(x.split()) <= max_len)]\n","    return df\n","\n","# Apply sentence length filtering for Hindi dataset\n","hindi_df = filter_sentence_length(hindi_df, 'Sentence')\n","\n","# Save the preprocessed Hindi dataset\n","hindi_df.to_csv('hindi_preprocessed.csv', index=False)\n","\n","print(f'Preprocessed Hindi dataset saved with {len(hindi_df)} rows.')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sUTyOTytDZjR","outputId":"9f181770-1d12-4dab-da66-05f80a7691e3","executionInfo":{"status":"ok","timestamp":1727412378904,"user_tz":-330,"elapsed":5696,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Preprocessed Hindi dataset saved with 198971 rows.\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import string\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","\n","# Load the English stopwords\n","english_stopwords = set(['the'])\n","\n","# Load the English dataset\n","english_df = pd.read_csv('english_with_intensity.csv')\n","\n","# Function to normalize text\n","def normalize_text(text):\n","    text = text.lower()  # Lowercase the text\n","    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)  # Remove punctuation and special characters\n","    text = re.sub(r'\\d+', '', text)  # Remove numbers\n","    text = text.strip()  # Remove extra whitespace\n","    return text\n","\n","# Function to remove stopwords for English\n","def remove_stopwords(text):\n","    return ' '.join([word for word in text.split() if word not in english_stopwords])\n","\n","# Apply text normalization and stopword removal for English dataset\n","def preprocess_english(df):\n","    df['Sentence'] = df['Sentence'].apply(normalize_text)\n","    df['Sentence'] = df['Sentence'].apply(remove_stopwords)\n","    return df\n","\n","# Preprocess English dataset\n","english_df = preprocess_english(english_df)\n","\n","# Function to filter sentences based on length (between 3 and 50 words)\n","def filter_sentence_length(df, column_name, min_len=3, max_len=50):\n","    df = df[df[column_name].apply(lambda x: min_len <= len(x.split()) <= max_len)]\n","    return df\n","\n","# Apply sentence length filtering for English dataset\n","english_df = filter_sentence_length(english_df, 'Sentence')\n","\n","# Save the preprocessed English dataset\n","english_df.to_csv('english_preprocessed.csv', index=False)\n","\n","print(f'Preprocessed English dataset saved with {len(english_df)} rows.')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iHvuvttLDbb4","outputId":"aa97ae77-f6c1-4b11-bb75-9868261dc53b","executionInfo":{"status":"ok","timestamp":1727412380583,"user_tz":-330,"elapsed":1735,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Preprocessed English dataset saved with 196185 rows.\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import string\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","\n","# Custom stopwords for Marathi\n","marathi_stopwords = set(['आहे', 'मध्ये', 'की', 'तो', 'आणि'])  # Add more as needed\n","\n","# Load the Marathi dataset\n","marathi_df = pd.read_csv('marathi_with_intensity.csv')\n","\n","# Function to normalize text\n","def normalize_text(text):\n","    text = text.lower()  # Lowercase the text\n","    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)  # Remove punctuation and special characters\n","    text = re.sub(r'\\d+', '', text)  # Remove numbers\n","    text = text.strip()  # Remove extra whitespace\n","    return text\n","\n","# Function to remove stopwords for Marathi\n","def remove_stopwords(text):\n","    return ' '.join([word for word in text.split() if word not in marathi_stopwords])\n","\n","# Apply text normalization and stopword removal for Marathi dataset\n","def preprocess_marathi(df):\n","    df['Sentence'] = df['Sentence'].apply(normalize_text)\n","    df['Sentence'] = df['Sentence'].apply(remove_stopwords)\n","    return df\n","\n","# Preprocess Marathi dataset\n","marathi_df = preprocess_marathi(marathi_df)\n","\n","# Function to filter sentences based on length (between 3 and 50 words)\n","def filter_sentence_length(df, column_name, min_len=3, max_len=50):\n","    df = df[df[column_name].apply(lambda x: min_len <= len(x.split()) <= max_len)]\n","    return df\n","\n","# Apply sentence length filtering for Marathi dataset\n","marathi_df = filter_sentence_length(marathi_df, 'Sentence')\n","\n","# Save the preprocessed Marathi dataset\n","marathi_df.to_csv('marathi_preprocessed.csv', index=False)\n","\n","print(f'Preprocessed Marathi dataset saved with {len(marathi_df)} rows.')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0PKpXnlXDbTB","outputId":"24090eac-8db2-43cd-f723-153a54243361","executionInfo":{"status":"ok","timestamp":1727412385284,"user_tz":-330,"elapsed":4708,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Preprocessed Marathi dataset saved with 199973 rows.\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import string\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","\n","# Custom stopwords for Gujarati\n","gujarati_stopwords = set(['છે', 'કે', 'હું', 'છો', 'આ', 'તે'])  # Add more as needed\n","\n","# Load the Gujarati dataset\n","gujrati_df = pd.read_csv('gujrati_with_intensity.csv')\n","\n","# Function to normalize text\n","def normalize_text(text):\n","    text = text.lower()  # Lowercase the text\n","    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)  # Remove punctuation and special characters\n","    text = re.sub(r'\\d+', '', text)  # Remove numbers\n","    text = text.strip()  # Remove extra whitespace\n","    return text\n","\n","# Function to remove stopwords for Gujarati\n","def remove_stopwords(text):\n","    return ' '.join([word for word in text.split() if word not in gujarati_stopwords])\n","\n","# Apply text normalization and stopword removal for Gujarati dataset\n","def preprocess_gujarati(df):\n","    df['Sentence'] = df['Sentence'].apply(normalize_text)\n","    df['Sentence'] = df['Sentence'].apply(remove_stopwords)\n","    return df\n","\n","# Preprocess Gujarati dataset\n","gujrati_df = preprocess_gujarati(gujrati_df)\n","\n","# Function to filter sentences based on length (between 3 and 50 words)\n","def filter_sentence_length(df, column_name, min_len=3, max_len=50):\n","    df = df[df[column_name].apply(lambda x: min_len <= len(x.split()) <= max_len)]\n","    return df\n","\n","# Apply sentence length filtering for Gujarati dataset\n","gujrati_df = filter_sentence_length(gujrati_df, 'Sentence')\n","\n","# Save the preprocessed Gujarati dataset\n","gujrati_df.to_csv('gujarati_preprocessed.csv', index=False)\n","\n","print(f'Preprocessed Gujarati dataset saved with {len(gujrati_df)} rows.')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nThQKqS9DbCi","outputId":"911dd199-acf1-4575-95ad-1eb8c48b4048","executionInfo":{"status":"ok","timestamp":1727412389162,"user_tz":-330,"elapsed":3885,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Preprocessed Gujarati dataset saved with 199998 rows.\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Step 1: Load and Sample 50,000 Rows from Each Preprocessed Dataset\n","\n","# Load the preprocessed datasets\n","hindi_df = pd.read_csv('hindi_preprocessed.csv')\n","english_df = pd.read_csv('english_preprocessed.csv')\n","marathi_df = pd.read_csv('marathi_preprocessed.csv')\n","gujrati_df = pd.read_csv('gujarati_preprocessed.csv')\n","\n","# Function to sample 50,000 rows from each dataset with equal sentiment distribution\n","def sample_data(df, n=50000):\n","    # Ensure equal proportion of sentiments\n","    sentiment_counts = df['Sentiment'].value_counts().min()  # Get the smallest class size\n","    samples_per_class = min(n // df['Sentiment'].nunique(), sentiment_counts)\n","\n","    df_sampled = df.groupby('Sentiment', group_keys=False).apply(lambda x: x.sample(samples_per_class, random_state=42))\n","    return df_sampled.sample(frac=1).reset_index(drop=True)\n","\n","# Sample 50,000 rows from each dataset\n","hindi_sampled = sample_data(hindi_df)\n","english_sampled = sample_data(english_df)\n","marathi_sampled = sample_data(marathi_df)\n","gujrati_sampled = sample_data(gujrati_df)\n","\n","# Combine the sampled datasets\n","combined_df = pd.concat([hindi_sampled, english_sampled, marathi_sampled, gujrati_sampled])\n","combined_df = combined_df.sample(frac=1).reset_index(drop=True)  # Shuffle the data\n","\n","print(f'Combined dataset has {len(combined_df)} rows.')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PLP5xvoVD-bb","outputId":"bc1e3aab-4281-4298-a94b-cc4eefa1f01a","executionInfo":{"status":"ok","timestamp":1727412391651,"user_tz":-330,"elapsed":2525,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Combined dataset has 199992 rows.\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Step 2: Apply TF-IDF Vectorization on the 'Sentence' column (limit features for memory efficiency)\n","tfidf_vectorizer = TfidfVectorizer(max_features=3000)  # Limiting features to reduce memory usage\n","X_tfidf = tfidf_vectorizer.fit_transform(combined_df['Sentence'])  # Keep the matrix in sparse format\n","\n","print(f'TF-IDF matrix shape: {X_tfidf.shape}')\n","\n","# Step 3: Encode Sentiment and Emotion Labels\n","\n","# Check for missing values and drop them\n","combined_df = combined_df.dropna(subset=['Sentiment', 'Emotion'])\n","\n","# Convert Sentiment and Emotion columns to string, if not already\n","combined_df['Sentiment'] = combined_df['Sentiment'].astype(str)\n","combined_df['Emotion'] = combined_df['Emotion'].astype(str)\n","\n","# Apply LabelEncoder to Sentiment and Emotion columns\n","sentiment_encoder = LabelEncoder()\n","emotion_encoder = LabelEncoder()\n","\n","combined_df['Sentiment'] = sentiment_encoder.fit_transform(combined_df['Sentiment'])\n","combined_df['Emotion'] = emotion_encoder.fit_transform(combined_df['Emotion'])\n","\n","# Prepare the labels\n","y_sentiment = combined_df['Sentiment']\n","y_emotion = combined_df['Emotion']\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cN1sbZpGF1wK","outputId":"0cc9f9b2-a387-4aad-87c1-d213a32b9046","executionInfo":{"status":"ok","timestamp":1727412393275,"user_tz":-330,"elapsed":1633,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["TF-IDF matrix shape: (199992, 3000)\n"]}]},{"cell_type":"code","source":["# Step 4: Fuzzify the Intensity Column\n","\n","# Fuzzify the Intensity column\n","def fuzzify_intensity(intensity):\n","    if intensity == 0:\n","        return np.array([1, 0, 0])  # Low intensity\n","    elif intensity == 1:\n","        return np.array([0, 1, 0])  # Medium intensity\n","    elif intensity == 2:\n","        return np.array([0, 0, 1])  # High intensity\n","\n","fuzzified_intensities = np.array([fuzzify_intensity(i) for i in combined_df['Intensity']])\n","\n","# Step 5: Split Data into Training, Validation, and Test Sets\n","\n","# Split fuzzified intensity values into training, validation, and test sets\n","fuzzy_train, fuzzy_temp = train_test_split(fuzzified_intensities, test_size=0.3, random_state=42)\n","fuzzy_val, fuzzy_test = train_test_split(fuzzy_temp, test_size=0.5, random_state=42)\n","\n","# Split data into training, validation, and test sets for the text TF-IDF features and labels\n","X_train_text, X_temp_text, y_train_sentiment, y_temp_sentiment, y_train_emotion, y_temp_emotion = train_test_split(\n","    X_tfidf, y_sentiment, y_emotion, test_size=0.3, random_state=42)\n","\n","X_val_text, X_test_text, y_val_sentiment, y_test_sentiment, y_val_emotion, y_test_emotion = train_test_split(\n","    X_temp_text, y_temp_sentiment, y_temp_emotion, test_size=0.5, random_state=42)\n"],"metadata":{"id":"Nd4OW0-SF4Jg","executionInfo":{"status":"ok","timestamp":1727412393737,"user_tz":-330,"elapsed":467,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Concatenate\n","\n","# Input layers for text (TF-IDF features) and fuzzified intensity (fuzzy logic)\n","input_text = Input(shape=(X_train_text.shape[1],), sparse=True, name='text_input')  # Use sparse input\n","input_fuzzy = Input(shape=(3,), name='fuzzy_input')  # Fuzzy input: Low, Medium, High intensity\n","\n","# Dense layers for feature extraction from text\n","dense_layer = Dense(128, activation='relu')(input_text)\n","dense_layer = Dense(64, activation='relu')(dense_layer)\n","\n","# Combine Dense output with fuzzy logic input (fuzzified intensity values)\n","concat_layer = Concatenate()([dense_layer, input_fuzzy])\n","\n","# Fully connected layers for sentiment and emotion classification\n","dense_sentiment = Dense(64, activation='relu')(concat_layer)\n","output_sentiment = Dense(3, activation='softmax', name='sentiment_output')(dense_sentiment)\n","\n","dense_emotion = Dense(64, activation='relu')(concat_layer)\n","output_emotion = Dense(9, activation='softmax', name='emotion_output')(dense_emotion)\n","\n","# Build the final model\n","model = Model(inputs=[input_text, input_fuzzy], outputs=[output_sentiment, output_emotion])\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss={'sentiment_output': 'sparse_categorical_crossentropy', 'emotion_output': 'sparse_categorical_crossentropy'},\n","              metrics={'sentiment_output': 'accuracy', 'emotion_output': 'accuracy'})\n","\n","# Show model summary\n","model.summary()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"id":"mrRSXo_GF8e3","outputId":"d7b1a415-d280-4f08-f0b1-5e4583899fbe","executionInfo":{"status":"ok","timestamp":1727412404441,"user_tz":-330,"elapsed":10707,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}}},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ text_input (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3000\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m384,128\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m8,256\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ fuzzy_input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ -                      │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m67\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n","│                           │                        │                │ fuzzy_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m4,352\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m4,352\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ sentiment_output (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m195\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ emotion_output (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │            \u001b[38;5;34m585\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n","└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ text_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3000</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">384,128</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ fuzzy_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">67</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n","│                           │                        │                │ fuzzy_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ sentiment_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ emotion_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">585</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n","└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m401,868\u001b[0m (1.53 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">401,868</span> (1.53 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m401,868\u001b[0m (1.53 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">401,868</span> (1.53 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["# Convert the sparse matrices (X_train_text, X_val_text, X_test_text) to dense format during training\n","X_train_text_dense = X_train_text.toarray()  # Convert sparse to dense\n","X_val_text_dense = X_val_text.toarray()      # Convert sparse to dense\n","X_test_text_dense = X_test_text.toarray()    # Convert sparse to dense\n"],"metadata":{"id":"XiafVvbmF_SQ","executionInfo":{"status":"ok","timestamp":1727412406352,"user_tz":-330,"elapsed":1920,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Step 7: Train the Model\n","\n","# Convert sparse matrices to dense\n","X_train_text_dense = X_train_text.toarray()  # Convert sparse to dense for training\n","X_val_text_dense = X_val_text.toarray()      # Convert sparse to dense for validation\n","\n","history = model.fit([X_train_text_dense, fuzzy_train],  # Pass dense matrix now\n","                    {'sentiment_output': y_train_sentiment, 'emotion_output': y_train_emotion},\n","                    epochs=50, batch_size=128, validation_data=([X_val_text_dense, fuzzy_val],\n","                    {'sentiment_output': y_val_sentiment, 'emotion_output': y_val_emotion}),\n","                    verbose=1)\n","\n","# Step 8: Save the trained model\n","model.save('dense_fuzzy_model_with_dense_input.h5')\n","\n","# Step 9: Evaluate the Model\n","\n","# Convert test sparse matrix to dense\n","X_test_text_dense = X_test_text.toarray()  # Convert sparse to dense for test evaluation\n","\n","test_results = model.evaluate([X_test_text_dense, fuzzy_test],\n","                              {'sentiment_output': y_test_sentiment, 'emotion_output': y_test_emotion})\n","\n","print(f'Test loss: {test_results[0]}')\n","print(f'Sentiment accuracy: {test_results[1]}')\n","print(f'Emotion accuracy: {test_results[2]}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":428},"id":"OsJTm3q2GjaY","outputId":"24dbd31d-6899-4645-99a1-dc66ef82c442","executionInfo":{"status":"error","timestamp":1727412429711,"user_tz":-330,"elapsed":23363,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - emotion_output_accuracy: 0.7937 - loss: 1.2088 - sentiment_output_accuracy: 0.7995"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-e7bf2d8e782a>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_val_text_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# Convert sparse to dense for validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m history = model.fit([X_train_text_dense, fuzzy_train],  # Pass dense matrix now\n\u001b[0m\u001b[1;32m      8\u001b[0m                     \u001b[0;34m{\u001b[0m\u001b[0;34m'sentiment_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train_sentiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'emotion_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train_emotion\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     epochs=50, batch_size=128, validation_data=([X_val_text_dense, fuzzy_val],\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    341\u001b[0m                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     )\n\u001b[0;32m--> 343\u001b[0;31m                 val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m    344\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m                 \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m                 \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","import numpy as np\n","\n","# Step 7: Train the Model\n","\n","# Convert sparse matrices to dense\n","X_train_text_dense = X_train_text.toarray()  # Convert sparse to dense for training\n","X_val_text_dense = X_val_text.toarray()      # Convert sparse to dense for validation\n","\n","history = model.fit([X_train_text_dense, fuzzy_train],  # Pass dense matrix now\n","                    {'sentiment_output': y_train_sentiment, 'emotion_output': y_train_emotion},\n","                    epochs=1, batch_size=128, validation_data=([X_val_text_dense, fuzzy_val],\n","                    {'sentiment_output': y_val_sentiment, 'emotion_output': y_val_emotion}),\n","                    verbose=1)\n","\n","# Step 8: Save the trained model\n","model.save('dense_fuzzy_model_with_dense_input.h5')\n","\n","# Step 9: Evaluate the Model\n","\n","# Convert test sparse matrix to dense\n","X_test_text_dense = X_test_text.toarray()  # Convert sparse to dense for test evaluation\n","\n","test_results = model.evaluate([X_test_text_dense, fuzzy_test],\n","                              {'sentiment_output': y_test_sentiment, 'emotion_output': y_test_emotion})\n","\n","print(f'Test loss: {test_results[0]}')\n","print(f'Sentiment accuracy: {test_results[1]}')\n","print(f'Emotion accuracy: {test_results[2]}')\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","import numpy as np\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","import numpy as np\n","\n","# Step 10: Predictions and Metrics\n","\n","# Get predictions for the test data\n","y_pred_sentiment_prob, y_pred_emotion_prob = model.predict([X_test_text_dense, fuzzy_test])\n","\n","# Convert predicted probabilities to class labels\n","y_pred_sentiment = np.argmax(y_pred_sentiment_prob, axis=1)\n","y_pred_emotion = np.argmax(y_pred_emotion_prob, axis=1)\n","\n","# If y_test_sentiment and y_test_emotion are already class labels (not one-hot encoded)\n","y_true_sentiment = y_test_sentiment if y_test_sentiment.ndim == 1 else np.argmax(y_test_sentiment, axis=1)\n","y_true_emotion = y_test_emotion if y_test_emotion.ndim == 1 else np.argmax(y_test_emotion, axis=1)\n","\n","# Sentiment Metrics\n","accuracy_sentiment = accuracy_score(y_true_sentiment, y_pred_sentiment)\n","precision_sentiment = precision_score(y_true_sentiment, y_pred_sentiment, average='macro')\n","recall_sentiment = recall_score(y_true_sentiment, y_pred_sentiment, average='macro')\n","f1_sentiment = f1_score(y_true_sentiment, y_pred_sentiment, average='macro')\n","conf_matrix_sentiment = confusion_matrix(y_true_sentiment, y_pred_sentiment)\n","\n","# Emotion Metrics\n","accuracy_emotion = accuracy_score(y_true_emotion, y_pred_emotion)\n","precision_emotion = precision_score(y_true_emotion, y_pred_emotion, average='macro')\n","recall_emotion = recall_score(y_true_emotion, y_pred_emotion, average='macro')\n","f1_emotion = f1_score(y_true_emotion, y_pred_emotion, average='macro')\n","conf_matrix_emotion = confusion_matrix(y_true_emotion, y_pred_emotion)\n","\n","# Output the evaluation in the desired format\n","\n","# Sentiment Model Evaluation\n","print(\"Sentiment Model Evaluation:\")\n","print(f\"Accuracy: {accuracy_sentiment}\")\n","print(f\"Precision: {precision_sentiment}\")\n","print(f\"Recall: {recall_sentiment}\")\n","print(f\"F1 Score: {f1_sentiment}\")\n","print(\"Confusion Matrix:\")\n","print(conf_matrix_sentiment)\n","\n","# Emotion Model Evaluation\n","print(\"\\nEmotion Model Evaluation:\")\n","print(f\"Accuracy: {accuracy_emotion}\")\n","print(f\"Precision: {precision_emotion}\")\n","print(f\"Recall: {recall_emotion}\")\n","print(f\"F1 Score: {f1_emotion}\")\n","print(\"Confusion Matrix:\")\n","print(conf_matrix_emotion)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hEtC2JmJy6Di","executionInfo":{"status":"ok","timestamp":1727413312358,"user_tz":-330,"elapsed":26824,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}},"outputId":"8e348256-78fc-4e34-f11b-a229310eaffa"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - emotion_output_accuracy: 0.9874 - loss: 0.0642 - sentiment_output_accuracy: 0.9872 - val_emotion_output_accuracy: 0.9519 - val_loss: 0.3385 - val_sentiment_output_accuracy: 0.9518\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - emotion_output_accuracy: 0.9507 - loss: 0.3456 - sentiment_output_accuracy: 0.9507\n","Test loss: 0.33865252137184143\n","Sentiment accuracy: 0.9504650235176086\n","Emotion accuracy: 0.9503650069236755\n","\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n","Sentiment Model Evaluation:\n","Accuracy: 0.9503650121670723\n","Precision: 0.9504801103489265\n","Recall: 0.9503683421129004\n","F1 Score: 0.9504101208525849\n","Confusion Matrix:\n","[[9595  190  207]\n"," [ 154 9401  390]\n"," [ 135  413 9514]]\n","\n","Emotion Model Evaluation:\n","Accuracy: 0.9504650155005167\n","Precision: 0.9508110229640104\n","Recall: 0.950295850819044\n","F1 Score: 0.9504778326091997\n","Confusion Matrix:\n","[[3225    0    0   31    1   91    1    0    0]\n"," [   0 3103    0    0    0    0   94   49    1]\n"," [   0    0 3160    0  154    0    0    0   60]\n"," [  38    0    1 3237    0   68    0    0    0]\n"," [   0    0  225    0 2980    0    0    0   34]\n"," [  75    0    0   51    0 3303    0    0    0]\n"," [   0   46    1    0    0    0 3227  120    0]\n"," [   0   50    0    0    0    0  145 3027    0]\n"," [   0    0  112    0   38    0    0    0 3251]]\n"]}]},{"cell_type":"code","source":["# Save the trained model\n","model.save('dense_fuzzy_model_with_dense_input.h5')  # Save the model for future use\n","\n","print(\"Model saved as 'dense_fuzzy_model_with_dense_input.h5'.\")\n"],"metadata":{"id":"ZLSCGZ6yHizp","executionInfo":{"status":"aborted","timestamp":1727412429713,"user_tz":-330,"elapsed":8,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Load the previously saved model\n","model = tf.keras.models.load_model('dense_fuzzy_model_with_dense_input.h5')\n"],"metadata":{"id":"HBaHKt5pH9P3","executionInfo":{"status":"aborted","timestamp":1727412429713,"user_tz":-330,"elapsed":8,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# Assuming combined_df contains the dataset with 'Sentence', 'Sentiment', 'Emotion'\n","# Randomly sample 50 sentences from the dataset\n","random_samples = combined_df.sample(50, random_state=42)\n","\n","# Prepare the features (text) and labels (sentiment, emotion) for these samples\n","X_random_text = tfidf_vectorizer.transform(random_samples['Sentence'])  # TF-IDF vectorization (sparse)\n","y_true_sentiment = random_samples['Sentiment'].values\n","y_true_emotion = random_samples['Emotion'].values\n","\n","# Convert the sparse matrix to dense (since model expects dense)\n","X_random_text_dense = X_random_text.toarray()\n"],"metadata":{"id":"rsQuyPN6IBX8","executionInfo":{"status":"aborted","timestamp":1727412429713,"user_tz":-330,"elapsed":8,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply fuzzification on each intensity value\n","fuzzy_intensities_random = np.array([fuzzify_intensity(i) for i in random_samples['Intensity']])\n","\n","# Predict using the loaded model\n","predictions = model.predict([X_random_text_dense, fuzzy_intensities_random])\n","\n","# Extract sentiment and emotion predictions\n","y_pred_sentiment = np.argmax(predictions[0], axis=1)\n","y_pred_emotion = np.argmax(predictions[1], axis=1)\n"],"metadata":{"id":"Zn4LZnSiIDeE","executionInfo":{"status":"aborted","timestamp":1727412429713,"user_tz":-330,"elapsed":7,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Plot line graph for Sentiment\n","plt.figure(figsize=(12, 6))\n","plt.plot(range(50), y_true_sentiment, label='True Sentiment', marker='o')\n","plt.plot(range(50), y_pred_sentiment, label='Predicted Sentiment', marker='x')\n","plt.title('True vs Predicted Sentiment')\n","plt.xlabel('Sample Index')\n","plt.ylabel('Sentiment Label')\n","plt.legend()\n","plt.show()\n","\n","# Plot line graph for Emotion\n","plt.figure(figsize=(12, 6))\n","plt.plot(range(50), y_true_emotion, label='True Emotion', marker='o')\n","plt.plot(range(50), y_pred_emotion, label='Predicted Emotion', marker='x')\n","plt.title('True vs Predicted Emotion')\n","plt.xlabel('Sample Index')\n","plt.ylabel('Emotion Label')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"EYJtEWBdIRKv","executionInfo":{"status":"aborted","timestamp":1727412429713,"user_tz":-330,"elapsed":7,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Double Bar Plot for Sentiment\n","plt.figure(figsize=(12, 6))\n","indices = np.arange(50)  # Indices for 50 samples\n","width = 0.35  # Width of the bars\n","\n","plt.bar(indices, y_true_sentiment, width=width, label='True Sentiment', color='b')\n","plt.bar(indices + width, y_pred_sentiment, width=width, label='Predicted Sentiment', color='r')\n","\n","plt.xlabel('Sample Index')\n","plt.ylabel('Sentiment Label')\n","plt.title('True vs Predicted Sentiment (Double Bar Plot)')\n","plt.legend()\n","plt.show()\n","\n","# Double Bar Plot for Emotion\n","plt.figure(figsize=(12, 6))\n","plt.bar(indices, y_true_emotion, width=width, label='True Emotion', color='b')\n","plt.bar(indices + width, y_pred_emotion, width=width, label='Predicted Emotion', color='r')\n","\n","plt.xlabel('Sample Index')\n","plt.ylabel('Emotion Label')\n","plt.title('True vs Predicted Emotion (Double Bar Plot)')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"vfRjNSliIctQ","executionInfo":{"status":"aborted","timestamp":1727412429713,"user_tz":-330,"elapsed":7,"user":{"displayName":"Veena Powle","userId":"09019381879046108764"}}},"execution_count":null,"outputs":[]}]}