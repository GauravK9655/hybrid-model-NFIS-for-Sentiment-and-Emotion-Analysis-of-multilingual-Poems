from flask import Flask, request, render_template
import tensorflow as tf
import numpy as np
import re
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Layer

# Define custom Attention layer with proper serialization and keyword argument handling
class AttentionLayer(Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def call(self, inputs, **kwargs):
        # Implement attention logic here if needed
        return inputs  # Placeholder for attention mechanism

    def get_config(self):
        config = super(AttentionLayer, self).get_config()
        return config

    @classmethod
    def from_config(cls, config):
        return cls(**config)

# Define custom GetItem layer with proper serialization and keyword argument handling
class GetItem(Layer):
    def __init__(self, index=None, **kwargs):
        super(GetItem, self).__init__(**kwargs)
        self.index = index

    def call(self, inputs, **kwargs):
        # Ensure slicing is handled correctly as a keyword argument
        if self.index is not None:
            return tf.gather(inputs, indices=self.index, axis=1)
        return inputs

    def get_config(self):
        config = super(GetItem, self).get_config()
        config.update({"index": self.index})
        return config

    @classmethod
    def from_config(cls, config):
        return cls(**config)

# Flask app setup
app = Flask(__name__)

# Load the model with custom layers and objects
custom_objects = {'AttentionLayer': AttentionLayer, 'GetItem': GetItem}
model = tf.keras.models.load_model('lstm_attention_model.h5', custom_objects=custom_objects)

# Preprocess the text input for prediction
def preprocess_text(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text.lower())  # Basic text cleaning
    tokenizer = Tokenizer(num_words=10000)  # Assuming a maximum of 10k words in the vocab
    tokenizer.fit_on_texts([text])  # Use the tokenizer fitted during training for actual implementation
    sequences = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(sequences, maxlen=100, padding='post')  # Adjust maxlen according to your model's input shape
    return padded

# Home route to render the input form
@app.route('/')
def home():
    return render_template('index.html')

# Prediction route to handle form submission and display result
@app.route('/predict', methods=['POST'])
def predict():
    input_text = request.form['text']
    preprocessed_text = preprocess_text(input_text)
    
    # Make prediction using the loaded model
    prediction = model.predict(preprocessed_text)
    sentiment = np.argmax(prediction, axis=1)[0]  # Get the predicted sentiment/emotion
    
    return render_template('result.html', sentiment=sentiment)

if __name__ == '__main__':
    # Optionally suppress TensorFlow warnings
    import os
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings about optimizations
    
    # Run the Flask app
    app.run(debug=True)
